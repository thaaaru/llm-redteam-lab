# Ollama Configuration for Red Team Lab
# These settings ensure localhost-only operation with resource limits

# Bind only to localhost (critical for air-gapped operation)
OLLAMA_HOST=127.0.0.1:11434

# Limit context window to prevent resource exhaustion
OLLAMA_NUM_CTX=2048

# GPU settings (set to 0 for CPU-only mode)
OLLAMA_NUM_GPU=0

# Thread limit for CPU inference
OLLAMA_NUM_THREAD=4

# Memory limits (in bytes, 4GB for CPU mode)
OLLAMA_MAX_LOADED_MODELS=1

# Logging
OLLAMA_DEBUG=0

# Model storage path
OLLAMA_MODELS=/usr/share/ollama/.ollama/models

# Keep model loaded in memory (faster testing, but uses more RAM)
OLLAMA_KEEP_ALIVE=5m

# Request timeout (in seconds)
OLLAMA_REQUEST_TIMEOUT=120
